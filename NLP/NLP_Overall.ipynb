{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f733e07",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) — Comprehensive Guide\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01cac8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Natural Language Processing (NLP) is an exciting branch of Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. From voice assistants like Siri and Alexa to automatic translation services and sentiment analysis on social media, NLP is at the core of many technologies we use daily.\n",
    "\n",
    "This guide introduces key NLP concepts with clear explanations and simple, practical Python code examples. It is designed for beginners and developers looking to understand NLP fundamentals and build basic NLP applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Learn NLP?\n",
    "\n",
    "- **Unlock the power of human language:** Teach machines to read and understand text and speech.  \n",
    "- **Improve user experiences:** Build chatbots, translators, and recommendation systems.  \n",
    "- **Analyze huge text data:** Extract insights from reviews, articles, social media, and more.  \n",
    "- **Grow your career:** NLP skills are in high demand in AI, data science, and software development.  \n",
    "\n",
    "---\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "1. Importing Libraries and Preparing Text\n",
    "2. Text Cleaning and Preprocessing  \n",
    "3. Tokenization  \n",
    "4. Stemming and Lemmatization  \n",
    "5. Parts of Speech Tagging  \n",
    "6. Named Entity Recognition  \n",
    "7. Text Vectorization  \n",
    "8. Stop Words Removal  \n",
    "9. Word Embeddings  \n",
    "10. Practical NLP Project: Sentiment Analysis  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0cce8",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries and Preparing Text\n",
    "\n",
    "Before working on NLP tasks, you need essential Python libraries like `nltk` and `re` for text processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4a5983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\makmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\makmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\makmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\makmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary datasets (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"Natural Language Processing allows computers to understand human language!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b0f58",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "We import nltk for natural language tools and re for regular expressions (pattern matching). The nltk.download calls ensure you have the necessary datasets for tokenization, stopwords, lemmatization, and POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21339471",
   "metadata": {},
   "source": [
    "### 2. Text Cleaning Using Regular Expressions\n",
    "\n",
    "Cleaning text involves removing unwanted characters like numbers, punctuation, or special symbols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing allows computers to understand human language \n"
     ]
    }
   ],
   "source": [
    "clean_text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97845b59",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "\n",
    "- re.sub('[^a-zA-Z]', ' ', text) replaces everything except letters with a space.\n",
    "\n",
    "- .lower() converts text to lowercase for consistency.\n",
    "This prepares the text for easier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522bc78",
   "metadata": {},
   "source": [
    "### 3. Tokenization: Splitting Text into Words\n",
    "\n",
    "Tokenization breaks text into smaller pieces called tokens, usually words or sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4d450a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'allows', 'computers', 'to', 'understand', 'human', 'language']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\makmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(clean_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db5ea0",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "Tokenization is essential because most NLP models work on tokens rather than whole texts. word_tokenize splits the cleaned text into individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c6a64",
   "metadata": {},
   "source": [
    "### 4. Removing Stop Words\n",
    "\n",
    "Stop words are common words like \"is\", \"the\", \"and\" that often do not carry significant meaning and can be removed to reduce noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0ab4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'allows', 'computers', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca825898",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "Removing stop words helps focus on meaningful words and improves performance in many NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280c787",
   "metadata": {},
   "source": [
    "### 5. Stemming: Reduce Words to Root Form\n",
    "\n",
    "Stemming is a crude way to reduce words to their base or root by chopping off endings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6df7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'allow', 'comput', 'understand', 'human', 'languag']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d323145",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "Stemming algorithms like Porter Stemmer remove suffixes (“running” → “run”) but can produce non-words like “organizat” instead of “organization”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a55227",
   "metadata": {},
   "source": [
    "### 6. Lemmatization: More Accurate Word Base Forms\n",
    "\n",
    "Lemmatization reduces words to their dictionary form (lemma), using context and word meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3a2f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'allows', 'computer', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abac527",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "Lemmatization considers part of speech and results in real words (e.g., “better” → “good”). It is more linguistically informed than stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b93bee",
   "metadata": {},
   "source": [
    "### 7. Parts of Speech Tagging\n",
    "\n",
    "Assigning word roles like noun, verb, adjective helps understand sentence structure and meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f58c0b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\makmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('allows', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7c9bf",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "POS tags tell you if a word is a noun (NN), verb (VB), adjective (JJ), etc. This information is valuable for syntactic analysis and advanced NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fb4a7",
   "metadata": {},
   "source": [
    "### 8. Named Entity Recognition (NER) with spaCy\n",
    "\n",
    "NER finds and classifies real-world entities like people, locations, and organizations in text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e7ac30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Steve Jobs PERSON\n",
      "California GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Apple was founded by Steve Jobs in California.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e637e",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "NER extracts important information like company names, people’s names, places, dates, and monetary values, crucial for information extraction and knowledge graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ca128",
   "metadata": {},
   "source": [
    "### 9. Text Vectorization: Bag of Words Example\n",
    "\n",
    "Computers understand numbers better than text. Bag of Words (BoW) converts text into vectors based on word frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5920d53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computers' 'fun' 'helps' 'is' 'language' 'natural' 'processing' 'text'\n",
      " 'understand']\n",
      "[[0 1 0 1 1 1 1 0 0]\n",
      " [1 0 1 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Natural Language Processing is fun.\",\n",
    "          \"Text processing helps computers understand language.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60db57c",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "BoW counts how many times each word appears, ignoring grammar and order. It is simple but effective for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d74197",
   "metadata": {},
   "source": [
    "### 10. Word Embeddings with Word2Vec (Gensim)\n",
    "\n",
    "Word embeddings capture semantic meanings by representing words as dense vectors in multi-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b0c090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
      "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
      " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
      " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
      "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
      "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
      "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
      " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
      "  0.00302193  0.00358008]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1)\n",
    "\n",
    "print(model.wv['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d858c0",
   "metadata": {},
   "source": [
    "*Explanation:*\n",
    "Word2Vec learns relationships between words based on their context. Words with similar meanings have vectors close together, enabling tasks like analogy solving and similarity measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce514b19",
   "metadata": {},
   "source": [
    "## Practical NLP Project Example: Sentiment Analysis on Restaurant Reviews\n",
    "\n",
    "This example demonstrates the full NLP pipeline — from loading data to predicting sentiment with a machine learning model.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Importing Required Libraries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "We start by importing three essential libraries:\n",
    "\n",
    "- numpy for efficient numerical operations and handling arrays,\n",
    "\n",
    "- matplotlib.pyplot for data visualization (optional but useful for later analysis),\n",
    "\n",
    "- pandas for data loading and manipulation, particularly to read the dataset into a structured DataFrame format.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Loading the Dataset\n",
    "\n",
    "```python\n",
    "dataset = pd.read_csv('NLP\\Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "We load the dataset of restaurant reviews using pandas’ read_csv method.\n",
    "\n",
    "- The dataset file is a tab-separated values (TSV) file, so we specify the delimiter as a tab (\\t).\n",
    "\n",
    "- quoting = 3 tells pandas to ignore double quotes around text, which can sometimes interfere with parsing.\n",
    "\n",
    "- The resulting dataset DataFrame contains two main columns: the review text and the sentiment label (0 for negative, 1 for positive).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Text Cleaning and Preprocessing\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "print(corpus[:5])\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "This loop performs several important preprocessing steps on each review:\n",
    "\n",
    "- Cleaning: Using regular expressions to remove any characters that are not letters (removes punctuation, numbers, etc.) so the text is cleaner and more consistent.\n",
    "\n",
    "- Lowercasing: Converting all letters to lowercase to treat words like “Good” and “good” as the same token.\n",
    "\n",
    "- Tokenization: Splitting the text into individual words for processing.\n",
    "\n",
    "- Stopwords Removal: Common, less informative words like “the”, “is”, and “and” are removed. However, “not” is kept because it negates meaning and is important for sentiment.\n",
    "\n",
    "- Stemming: Each word is reduced to its root form to consolidate different forms of a word (e.g., “loved”, “loving” → “love”).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Creating the Bag of Words Model\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "- The Bag of Words (BoW) model transforms the cleaned text into a matrix of token counts.\n",
    "\n",
    "- CountVectorizer builds a vocabulary of the most frequent 1500 words across all reviews (using max_features=1500).\n",
    "\n",
    "- Each review is converted into a fixed-length vector where each position counts how many times a vocabulary word appears.\n",
    "\n",
    "- X is the feature matrix representing all reviews numerically, suitable for machine learning.\n",
    "\n",
    "- y extracts the sentiment labels (target values) from the dataset for supervised learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Splitting Data into Training and Test Sets\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "- We split the dataset into training and testing sets.\n",
    "\n",
    "- test_size=0.20 means 20% of data is held out for testing to evaluate the model’s performance on unseen data.\n",
    "\n",
    "- random_state=0 ensures reproducibility of the split.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Training the Naive Bayes Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "- We use a Gaussian Naive Bayes classifier, a probabilistic model well-suited for text classification.\n",
    "\n",
    "- Naive Bayes assumes feature independence and uses Bayes’ theorem to calculate the probability of each class given the input features.\n",
    "\n",
    "- The fit method trains the model on the training data, learning patterns that associate word counts with positive or negative sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Predicting the Test Set Results\n",
    "\n",
    "```python\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "- We use the trained model to predict sentiments on the test set.\n",
    "\n",
    "- Predictions (y_pred) are compared side-by-side with actual labels (y_test) to visually inspect model accuracy.\n",
    "\n",
    "- The concatenated output helps identify which predictions were correct or incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8: Evaluating Model Performance\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "- The confusion matrix shows true positives, true negatives, false positives, and false negatives in a matrix form.\n",
    "\n",
    "- accuracy_score gives the overall percentage of correct predictions.\n",
    "\n",
    "- Together, these metrics give a clear understanding of how well the sentiment analysis model performs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
